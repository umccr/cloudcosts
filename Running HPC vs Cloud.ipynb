{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Roman, want to field the back-of-the-envelope calculations?\n",
    "> \n",
    "A single tumor/normal sample is around 400GB of raw storage\n",
    "We expect an average of 5 samples / week for the next 12 months (but it's easy to scale up the calculations if needed)\n",
    "Data flow would be from the Cache -> S3 (~3-6 months storage period) and in parallel a copy to Glacier.\n",
    "> \n",
    "What is the storage cost per sample for the lifecycle?\n",
    "What is the cost to move a sample to NCI (data out) for processing?\n",
    "How long does it take to restore a sample from Glacier if we want to keep cost < 500 AUD?\n",
    "Storage costs on NCI are:\n",
    "> \n",
    "156\\$ / TB / year on active storage (S3 equivalent) or ~\\$30 per sample if kept for 6 months\n",
    "73\\$ / TB on (dual) tape archive or ~\\$30 per sample and year\n",
    "> \n",
    "Ignore compute for now, I don't have good numbers.\n",
    "[8:49] \n",
    "If you absolutely want to, we need machines with 4GB memory / core. A sample takes ~48h on 128 cores or ~6400 CPU hours.\n",
    "> \n",
    "[8:49] \n",
    "At \\$0.0260/CPU hour that's ~$165 AUD for the processing.\n",
    "> \n",
    "[8:50] \n",
    "But those numbers would only translate 1:1 if we could run bcbio on AWS which we can't. Any other runner will have different and likely better runtimes on AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding compute at NCI and its cost:\n",
    "\n",
    "\n",
    " ```Total Grant: 300.00 KSU\n",
    "Total Used:  200.00 KSU\n",
    "Total Avail: 100.00 KSU\n",
    "Bonus Used:  41.86 KSU```\n",
    "\n",
    "\n",
    "Going through ~100k Units a month right now or about 2600$ AUD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch S3 and EC2 pricing data from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted to Python 3 from: https://blog.rackspace.com/experimenting-aws-price-list-api\n",
    "import json, boto3, time, requests\n",
    "from collections import defaultdict\n",
    "\n",
    "AWS_SERVICES_IDX = 'https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/index.json'\n",
    "\n",
    "AWS_EC2_URL = 'https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/ap-southeast-2/index.json'\n",
    "AWS_S3_URL = 'https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonS3/current/ap-southeast-2/index.json'\n",
    "AWS_GLACIER_URL = 'https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonGlacier/current/ap-southeast-2/index.json'\n",
    "\n",
    "def read_aws_prices(service):\n",
    "  event = defaultdict()\n",
    "\n",
    "  if service == 's3':\n",
    "    offer = json.load(open('s3.json', 'r'))\n",
    "    prices = extract_s3_prices(offer)\n",
    "  elif service == 'glacier':\n",
    "    offer = json.load(open('glacier.json', 'r'))\n",
    "    prices = extract_s3_prices(offer)\n",
    "  elif service == 'ec2':\n",
    "    offer = json.load(open('ec2.json', 'r'))\n",
    "    prices = extract_ec2_prices(offer)\n",
    "\n",
    "  return prices\n",
    "\n",
    "def get_aws_prices(service):\n",
    "  event = defaultdict()\n",
    "\n",
    "  if service == 's3':\n",
    "    event['offerCode'] = 'AmazonS3'\n",
    "    offer = download_offer(event)\n",
    "    prices = extract_s3_prices(offer)\n",
    "  elif service == 'glacier':\n",
    "    event['offerCode'] = 'AmazonGlacier'\n",
    "    offer = download_offer(event)\n",
    "    prices = extract_s3_prices(offer)\n",
    "  elif service == 'ec2':\n",
    "    event['offerCode'] = 'AmazonEC2'\n",
    "    offer = download_offer(event)\n",
    "    prices = extract_ec2_prices(offer)\n",
    "\n",
    "  #upload_prices(prices)\n",
    "  return prices\n",
    "\n",
    "def download_offer(event):\n",
    "  if event['offerCode'] == 'AmazonS3':\n",
    "    URL = AWS_S3_URL\n",
    "  elif event['offerCode'] == 'AmazonEC2':\n",
    "    URL = AWS_EC2_URL\n",
    "  elif event['offerCode'] == 'AmazonGlacier':\n",
    "    URL = AWS_GLACIER_URL\n",
    "\n",
    "  response = requests.get(URL)\n",
    "  return json.loads(response.text)\n",
    "\n",
    "def filter_ec2_products(products):\n",
    "  filtered = []\n",
    "\n",
    "  # Only interested in shared tenancy, linux instances\n",
    "  for sku, product in products:\n",
    "    a = product['attributes']\n",
    "    if not ('locationType' in a and\n",
    "            'location' in a and\n",
    "            'tenancy' in a and\n",
    "            a['tenancy'] == \"Shared\" and\n",
    "            a['locationType'] == 'AWS Region' and\n",
    "            a['operatingSystem'] == 'Linux'):\n",
    "      continue\n",
    "\n",
    "    a['sku'] = sku\n",
    "    filtered.append(a)\n",
    "\n",
    "  return filtered\n",
    "\n",
    "def filter_s3_products(products):\n",
    "  filtered = []\n",
    "\n",
    "  for sku, product in products:\n",
    "    a = product['attributes']\n",
    "    if not ('usagetype' in a and\n",
    "            'fromLocation' in a and\n",
    "            'toLocation' in a and\n",
    "            'location' in a and\n",
    "            a['usagetype'] == \"APS2-DataTransfer-In-Bytes\" or\n",
    "            a['usagetype'] == \"APS2-DataTransfer-Out-Bytes\" and\n",
    "            a['toLocation'] == \"External\" and\n",
    "            a['fromLocation'] == 'Asia Pacific (Sydney)'):\n",
    "            # and\n",
    "            #a['toLocation'] == 'Asia Pacific (Sydney)'):\n",
    "      continue\n",
    "\n",
    "    a['sku'] = sku\n",
    "    filtered.append(a)\n",
    "\n",
    "  return filtered\n",
    "\n",
    "\n",
    "def extract_ec2_prices(offer):\n",
    "  terms = offer['terms']\n",
    "  products = offer['products'].items()\n",
    "\n",
    "  instances = {}\n",
    "  for a in filter_ec2_products(products):\n",
    "    term = list(terms['OnDemand'][a['sku']].items())[0][1]\n",
    "    cost = list(term['priceDimensions'].items())[0][1]\n",
    "    cost = cost['pricePerUnit']['USD']\n",
    "\n",
    "\n",
    "    info = {\"type\" : a['instanceType'], \"vcpu\" : a['vcpu'], \n",
    "            \"memory\" : a['memory'].split(\" \")[0], \"cost\" : cost}\n",
    "\n",
    "    if not a['location'] in instances:\n",
    "      instances[a['location']] = []\n",
    "\n",
    "    instances[a['location']].append(info)\n",
    "\n",
    "  return {'created': time.strftime(\"%c\"), 'published': offer['publicationDate'], \n",
    "          'instances': instances}\n",
    "\n",
    "def extract_s3_prices(offer):\n",
    "  terms = offer['terms']\n",
    "  products = offer['products'].items()\n",
    "\n",
    "  instances = {}\n",
    "  for a in filter_s3_products(products):\n",
    "    term = list(terms['OnDemand'][a['sku']].items())[0][1]\n",
    "    cost = list(term['priceDimensions'].items())[0][1]\n",
    "    cost = cost['pricePerUnit']['USD']\n",
    "    \n",
    "    info = {\"type\": a[\"usagetype\"], \"from\": a[\"fromLocation\"], \"to\": a[\"toLocation\"], \"cost\": float(cost)}\n",
    "    \n",
    "#    if not a['location'] in instances:\n",
    "#      instances[a['location']] = []\n",
    "    \n",
    "#    instances[a['location']].append(info)\n",
    "    \n",
    "  return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost': 0.14,\n",
       " 'from': 'Asia Pacific (Sydney)',\n",
       " 'to': 'External',\n",
       " 'type': 'APS2-DataTransfer-Out-Bytes'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S3 and Glacier cost in dollars per GB per month\n",
    "s3_ingress_unit_cost = read_aws_prices(\"s3\")\n",
    "glacier_ingress_unit_cost = read_aws_prices(\"glacier\")\n",
    "s3_ingress_unit_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hour granularity for time model\n",
    "# GiB granularity for space model\n",
    "\n",
    "sample_rate = 5/(24*7)\n",
    "storage_retention_policy = 6*30*24 # 6 months in hours\n",
    "\n",
    "sample_size = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 ingress costs in USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cost of sending a single sample_size sample to S3\n",
    "s3_ingress_unit_cost['cost'] * sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample cost times 5 (samples per week) times 4 weeks a month\n",
    "sample_unit_cost = s3_ingress_unit_cost['cost'] * sample_size * 4 * 5\n",
    "sample_unit_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manual lookup on https://aws.amazon.com/s3/pricing/, sanity check\n",
    "assert(8000*0.025) == 200\n",
    "\n",
    "# So $200 dollar, ballpark on $320...ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yearly\n",
    "s3_ingress_unit_cost['cost'] * sample_size * 4 * 5 * 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"What is the storage cost per sample for the lifecycle?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S3 transfer cost per sample with Glacier storage in parallel\n",
    "320 + (sample_size * 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"What is the cost to move a sample to NCI (data out) for processing?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends. Let's first assume that we are not in a rush to retrieve the data so that we don't go Glacier \"expedited mode\" (expensive, urgent retrieval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.00000000000001"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.140 * sample_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
